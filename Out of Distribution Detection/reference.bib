@misc{liu2021energybased,
      title={Energy-based Out-of-distribution Detection}, 
      author={Weitang Liu and Xiaoyun Wang and John D. Owens and Yixuan Li},
      year={2021},
      eprint={2010.03759},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{kirichenko2020normalizing,
      title={Why Normalizing Flows Fail to Detect Out-of-Distribution Data}, 
      author={Polina Kirichenko and Pavel Izmailov and Andrew Gordon Wilson},
      year={2020},
      eprint={2006.08545},
      archivePrefix={arXiv},
      primaryClass={stat.ML}
}

@misc{choi2019waic,
      title={WAIC, but Why? Generative Ensembles for Robust Anomaly Detection}, 
      author={Hyunsun Choi and Eric Jang and Alexander A. Alemi},
      year={2019},
      eprint={1810.01392},
      archivePrefix={arXiv},
      primaryClass={stat.ML}
}

@misc{nalisnick2019deep,
      title={Do Deep Generative Models Know What They Don't Know?}, 
      author={Eric Nalisnick and Akihiro Matsukawa and Yee Whye Teh and Dilan Gorur and Balaji Lakshminarayanan},
      year={2019},
      eprint={1810.09136},
      archivePrefix={arXiv},
      primaryClass={stat.ML}
}

@misc{lee2018training,
      title={Training Confidence-calibrated Classifiers for Detecting Out-of-Distribution Samples}, 
      author={Kimin Lee and Honglak Lee and Kibok Lee and Jinwoo Shin},
      year={2018},
      eprint={1711.09325},
      archivePrefix={arXiv},
      primaryClass={stat.ML}
}


@InProceedings{chen2020generative,
  title = 	 {Generative Pretraining From Pixels},
  author =       {Chen, Mark and Radford, Alec and Child, Rewon and Wu, Jeffrey and Jun, Heewoo and Luan, David and Sutskever, Ilya},
  booktitle = 	 {Proceedings of the 37th International Conference on Machine Learning},
  pages = 	 {1691--1703},
  year = 	 {2020},
  editor = 	 {III, Hal Daumé and Singh, Aarti},
  volume = 	 {119},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {13--18 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v119/chen20s/chen20s.pdf},
  url = 	 {https://proceedings.mlr.press/v119/chen20s.html},
}

@misc{xiao2020likelihood,
      title={Likelihood Regret: An Out-of-Distribution Detection Score For Variational Auto-encoder}, 
      author={Zhisheng Xiao and Qing Yan and Yali Amit},
      year={2020},
      eprint={2003.02977},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{kaggle, 
    title={Dogs vs. cats}, 
    url={https://www.kaggle.com/c/dogs-vs-cats/discussion}, 
    journal={Kaggle}, 
    year={2013}, 
    month={Sep}
} 

@TECHREPORT{krizhevsky09learning,
    author = {Alex Krizhevsky},
    title = {Learning multiple layers of features from tiny images},
    institution = {},
    year = {2009}
}

@article{netzer2011reading,
  author = {Netzer, Yuval and Wang, Tao and Coates, Adam and Bissacco, Alessandro and Wu, Bo and Ng, Andrew Y.},
  biburl = {https://www.bibsonomy.org/bibtex/263f0dc176f197808682a84b6ec5fa1aa/kirk86},
  description = {nips2011_housenumbers.pdf},
  interhash = {ce0c85ac4759bcf1f5dc2deaa1caf304},
  intrahash = {63f0dc176f197808682a84b6ec5fa1aa},
  keywords = {datasets deep-learning},
  timestamp = {2020-01-20T12:23:18.000+0100},
  title = {Reading Digits in Natural Images with Unsupervised Feature Learning
},
  url = {http://ufldl.stanford.edu/housenumbers/nips2011_housenumbers.pdf},
  year = 2011
}

@inproceedings{le2015tiny,
  title={Tiny ImageNet Visual Recognition Challenge},
  author={Ya Le and Xuan S. Yang},
  year={2015}
}

@misc{yu2016lsun,
      title={LSUN: Construction of a Large-scale Image Dataset using Deep Learning with Humans in the Loop}, 
      author={Fisher Yu and Ari Seff and Yinda Zhang and Shuran Song and Thomas Funkhouser and Jianxiong Xiao},
      year={2016},
      eprint={1506.03365},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@INPROCEEDINGS{deng2009imagenet,
  author={Deng, Jia and Dong, Wei and Socher, Richard and Li, Li-Jia and Kai Li and Li Fei-Fei},
  booktitle={2009 IEEE Conference on Computer Vision and Pattern Recognition}, 
  title={ImageNet: A large-scale hierarchical image database}, 
  year={2009},
  volume={},
  number={},
  pages={248-255},
  doi={10.1109/CVPR.2009.5206848}
}

@misc{hendrycks2018baseline,
      title={A Baseline for Detecting Misclassified and Out-of-Distribution Examples in Neural Networks}, 
      author={Dan Hendrycks and Kevin Gimpel},
      year={2018},
      eprint={1610.02136},
      archivePrefix={arXiv},
      primaryClass={cs.NE}
}

@article{hendrycks2019deep,
  title={Deep Anomaly Detection with Outlier Exposure},
  author={Dan Hendrycks and Mantas Mazeika and Thomas G. Dietterich},
  journal={ArXiv},
  year={2019},
  volume={abs/1812.04606}
}

@inproceedings{hendrycks2019using,
  title={Using Self-Supervised Learning Can Improve Model Robustness and Uncertainty},
  author={Dan Hendrycks and Mantas Mazeika and Saurav Kadavath and Dawn Xiaodong Song},
  booktitle={NeurIPS},
  year={2019}
}

@article{tack2020csi,
  title={CSI: Novelty Detection via Contrastive Learning on Distributionally Shifted Instances},
  author={Jihoon Tack and Sangwoo Mo and Jongheon Jeong and Jinwoo Shin},
  journal={ArXiv},
  year={2020},
  volume={abs/2007.08176}
}

@article{yoon2021self,
  title={Self-Trained One-class Classification for Unsupervised Anomaly Detection},
  author={Jinsung Yoon and Kihyuk Sohn and Chun-Liang Li and Sercan {\"O}. Arik and Chen-Yu Lee and Tomas Pfister},
  journal={ArXiv},
  year={2021},
  volume={abs/2106.06115}
}

@misc{he2021masked,
      title={Masked Autoencoders Are Scalable Vision Learners}, 
      author={He, Kaiming and Chen, Xinlei and Xie, Saining and Li, Yanghao and Dollár, Piotr and Girshick, Ross},
      year={2021},
      eprint={2111.06377},
      archivePrefix={arXiv},
}

@article{devlin2019bert,
  title={BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding},
  author={Jacob Devlin and Ming-Wei Chang and Kenton Lee and Kristina Toutanova},
  journal={ArXiv},
  year={2019},
  volume={abs/1810.04805}
}

@article{chandola2009anomaly,
    author = {Chandola, Varun and Banerjee, Arindam and Kumar, Vipin},
    year = {2009},
    month = {07},
    pages = {},
    title = {Anomaly Detection: A Survey},
    volume = {41},
    journal = {ACM Comput. Surv.},
    doi = {10.1145/1541880.1541882}
}

@misc{liang2020enhancing,
      title={Enhancing The Reliability of Out-of-distribution Image Detection in Neural Networks}, 
      author={Shiyu Liang and Yixuan Li and R. Srikant},
      year={2020},
      eprint={1706.02690},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@inproceedings{lee2018simple,
    author = {Lee, Kimin and Lee, Kibok and Lee, Honglak and Shin, Jinwoo},
    title = {A Simple Unified Framework for Detecting Out-of-Distribution Samples and Adversarial Attacks},
    year = {2018},
    publisher = {Curran Associates Inc.},
    address = {Red Hook, NY, USA},
    abstract = {Detecting test samples drawn sufficiently far away from the training distribution statistically or adversarially is a fundamental requirement for deploying a good classifier in many real-world machine learning applications. However, deep neural networks with the softmax classifier are known to produce highly overconfident posterior distributions even for such abnormal samples. In this paper, we propose a simple yet effective method for detecting any abnormal samples, which is applicable to any pre-trained softmax neural classifier. We obtain the class conditional Gaussian distributions with respect to (low- and upper-level) features of the deep models under Gaussian discriminant analysis, which result in a confidence score based on the Mahalanobis distance. While most prior methods have been evaluated for detecting either out-of-distribution or adversarial samples, but not both, the proposed method achieves the state-of-the-art performances for both cases in our experiments. Moreover, we found that our proposed method is more robust in harsh cases, e.g., when the training dataset has noisy labels or small number of samples. Finally, we show that the proposed method enjoys broader usage by applying it to class-incremental learning: whenever out-of-distribution samples are detected, our classification rule can incorporate new classes well without further training deep models.},
    booktitle = {Proceedings of the 32nd International Conference on Neural Information Processing Systems},
    pages = {7167–7177},
    numpages = {11},
    location = {Montr\'{e}al, Canada},
    series = {NIPS'18}
}